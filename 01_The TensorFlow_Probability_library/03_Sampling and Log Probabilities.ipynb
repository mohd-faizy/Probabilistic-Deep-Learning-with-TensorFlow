{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cae8af82",
   "metadata": {},
   "source": [
    "# **Sampling and Log Probabilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f077cde",
   "metadata": {},
   "source": [
    "## 📑 Table of Contents\n",
    "\n",
    "- **1. [Foundation - What Are Sampling and Log Probabilities?](#1-foundation---what-are-sampling-and-log-probabilities)**\n",
    "    - 1.1 [The Big Picture](#11-the-big-picture)\n",
    "\n",
    "- **2. [Setup & Basic Concepts](#2-setup--basic-concepts)**\n",
    "    - 2.1 [Essential Imports & Setup](#21-essential-imports--setup)  \n",
    "    - 2.2 [Your First Sampling & Log Prob Operations](#22-your-first-sampling--log-prob-operations)\n",
    "\n",
    "- **3. [Basic Sampling Operations](#3-basic-sampling-operations)**\n",
    "    - 3.1 [Sampling from Batched Distributions](#31-sampling-from-batched-distributions)  \n",
    "    - 3.2 [Independent Wrapper for Joint Sampling](#32-independent-wrapper-for-joint-sampling)\n",
    "\n",
    "- **4. [Complex Multi-Dimensional Sampling](#4-complex-multi-dimensional-sampling)**\n",
    "    - 4.1 [Advanced Batch Structures](#41-advanced-batch-structures)  \n",
    "    - 4.2 [Multi-Dimensional Sampling](#42-multi-dimensional-sampling)\n",
    "\n",
    "- **5. [Log Probability Computations](#5-log-probability-computations)**\n",
    "    - 5.1 [Basic Log Probability Evaluation](#51-basic-log-probability-evaluation)  \n",
    "    - 5.2 [Vector Input Log Probability](#52-vector-input-log-probability)\n",
    "\n",
    "- **6. [Advanced Broadcasting & Shape Mechanics](#6-advanced-broadcasting--shape-mechanics)**\n",
    "    - 6.1 [Complex Broadcasting Patterns](#61-complex-broadcasting-patterns)  \n",
    "    - 6.2 [Multivariate Distribution with Independent](#62-multivariate-distribution-with-independent)\n",
    "\n",
    "- **7. [Professional Patterns & Best Practices](#7-professional-patterns--best-practices)**\n",
    "    - 7.1 [Efficient Sampling Strategies](#71-efficient-sampling-strategies)  \n",
    "    - 7.2 [Broadcasting Validation Utilities](#72-broadcasting-validation-utilities)  \n",
    "    - 7.3 [Probabilistic Training Loss Patterns](#73-probabilistic-training-loss-patterns)\n",
    "\n",
    "- **8. [Expert Applications](#8-expert-applications)**\n",
    "    - 8.1 [Monte Carlo Variational Inference](#81-monte-carlo-variational-inference)  \n",
    "    - 8.2 [Advanced Sampling Techniques](#82-advanced-sampling-techniques)\n",
    "\n",
    "- **9. [Complete Reference Guide](#9-complete-reference-guide)**\n",
    "    - 9.1 [Sampling Operations Reference](#91-sampling-operations-reference)  \n",
    "    - 9.2 [Log Probability Operations Reference](#92-log-probability-operations-reference)  \n",
    "    - 9.3 [Shape Rules for Sampling and Log Prob](#93-shape-rules-for-sampling-and-log-prob)\n",
    "\n",
    "- **[Learning Path](#learning-path)**\n",
    "\n",
    "- **[Final Notes](#final-notes)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106b9469",
   "metadata": {},
   "source": [
    "## 1. **Foundation - What Are Sampling and Log Probabilities?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8f319",
   "metadata": {},
   "source": [
    "### 1.1 **The Big Picture**\n",
    "Sampling and log probability computations are the **core computational operations** in TensorFlow Probability. Every probabilistic machine learning algorithm relies on these two fundamental operations: **drawing samples** from distributions and **evaluating probabilities** of observed data.\n",
    "\n",
    "**Key Insight**: While `sample()` generates random data for Monte Carlo estimation and training, `log_prob()` evaluates how likely observed data is under your probabilistic model—the foundation of maximum likelihood estimation and Bayesian inference.[1]\n",
    "\n",
    "**Mathematical Foundation**:\n",
    "- **Sampling**: Generate `x ~ p(x)` for Monte Carlo approximation\n",
    "- **Log Probability**: Compute `log p(x)` for stable numerical likelihood evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda5b75",
   "metadata": {},
   "source": [
    "## 2. **Setup & Basic Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cbd84d",
   "metadata": {},
   "source": [
    "### 2.1 **Essential Imports & Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b73974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing TensorFlow Probability library\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "\n",
    "# Standard alias for distributions\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21592a6b",
   "metadata": {},
   "source": [
    "### 2.2 **Your First Sampling & Log Prob Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e14ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Exponential(\"Exponential\", batch_shape=[2, 3], event_shape=[], dtype=float32)\n",
      "Batch shape: (2, 3)\n",
      "Event shape: ()\n"
     ]
    }
   ],
   "source": [
    "# Create a batched exponential distribution\n",
    "exp = tfd.Exponential(rate=[[1., 1.5, 0.8], [0.3, 0.4, 1.8]])\n",
    "print(exp)\n",
    "# Output: <tfp.distributions.Exponential 'Exponential' batch_shape=[2, 3] event_shape=[] dtype=float32>\n",
    "\n",
    "print(\"Batch shape:\", exp.batch_shape)  \n",
    "print(\"Event shape:\", exp.event_shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5d850",
   "metadata": {},
   "source": [
    "**Key Properties**:\n",
    "- **`batch_shape=[2, 3]`**: 2×3 = 6 independent exponential distributions\n",
    "- **`event_shape=[]`**: Each distribution produces scalar values\n",
    "- **`rate` parameter**: Different rates for each distribution in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9870771",
   "metadata": {},
   "source": [
    "## 3. **Basic Sampling Operations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef566b",
   "metadata": {},
   "source": [
    "### 3.1 **Sampling from Batched Distributions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f29e989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sample shape: (2, 3)\n",
      "Single sample:\n",
      " tf.Tensor(\n",
      "[[4.390422   2.7434108  0.16488166]\n",
      " [0.350755   2.5880651  0.47506964]], shape=(2, 3), dtype=float32)\n",
      "Multiple samples shape: (4, 2, 3)\n",
      "Multiple samples:\n",
      " tf.Tensor(\n",
      "[[[2.1462562  0.86743593 0.0618375 ]\n",
      "  [2.3312993  2.7058575  0.34583405]]\n",
      "\n",
      " [[0.7877366  0.6282713  0.04425894]\n",
      "  [0.20140766 4.1841908  0.49354032]]\n",
      "\n",
      " [[0.07714589 0.8290665  0.32057405]\n",
      "  [3.3196666  0.78857696 0.37590277]]\n",
      "\n",
      " [[0.05160386 0.35619614 0.31295168]\n",
      "  [0.9867175  0.29580456 0.33078858]]], shape=(4, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create batched exponential distribution\n",
    "exp = tfd.Exponential(rate=[[1., 1.5, 0.8], [0.3, 0.4, 1.8]])\n",
    "\n",
    "# Single sample from each distribution\n",
    "single_sample = exp.sample()\n",
    "print(\"Single sample shape:\", single_sample.shape)  # (2, 3)\n",
    "print(\"Single sample:\\n\", single_sample)\n",
    "\n",
    "# Multiple samples\n",
    "multiple_samples = exp.sample(4)  \n",
    "print(\"Multiple samples shape:\", multiple_samples.shape)  # (4, 2, 3)\n",
    "print(\"Multiple samples:\\n\", multiple_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f979deb",
   "metadata": {},
   "source": [
    "**Shape Analysis**:\n",
    "- **Single sample**: `shape = batch_shape + event_shape = (2, 3) + () = (2, 3)`\n",
    "- **Multiple samples**: `shape = sample_shape + batch_shape + event_shape = (4,) + (2, 3) + () = (4, 2, 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ca11e",
   "metadata": {},
   "source": [
    "### 3.1 **Independent Wrapper for Joint Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8cbcb6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Independent(\"IndependentExponential\", batch_shape=[2], event_shape=[3], dtype=float32)\n",
      "Joint samples shape: (4, 2, 3)\n",
      "Joint samples:\n",
      " tf.Tensor(\n",
      "[[[2.6272154  2.051555   3.347836  ]\n",
      "  [1.6800407  0.51907015 0.9736745 ]]\n",
      "\n",
      " [[1.0240827  0.8342139  2.1261241 ]\n",
      "  [0.98191    1.8131511  0.6493894 ]]\n",
      "\n",
      " [[0.26463446 0.08187366 5.945581  ]\n",
      "  [5.35103    5.085251   0.5521324 ]]\n",
      "\n",
      " [[0.81465316 0.52829653 0.18442416]\n",
      "  [2.2659454  1.7780305  0.37588385]]], shape=(4, 2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Convert batched to multivariate using Independent\n",
    "exp = tfd.Exponential(rate=[[1., 1.5, 0.8], [0.3, 0.4, 1.8]])\n",
    "ind_exp = tfd.Independent(exp)  # Default reinterpreted_batch_ndims=1\n",
    "print(ind_exp)\n",
    "# Output: <tfp.distributions.Independent 'IndependentExponential' batch_shape=[2] event_shape=[2] dtype=float32>\n",
    "\n",
    "# Sample from Independent distribution\n",
    "joint_samples = ind_exp.sample(4)\n",
    "print(\"Joint samples shape:\", joint_samples.shape)  # (4, 2, 3)\n",
    "print(\"Joint samples:\\n\", joint_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f998e5c5",
   "metadata": {},
   "source": [
    "**Shape Transformation**:\n",
    "- **Original**: `batch_shape=[2, 3], event_shape=[]` → 6 independent scalars\n",
    "- **After Independent**: `batch_shape=, event_shape=` → 2 joint 3D vectors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d13e34",
   "metadata": {},
   "source": [
    "## 4. **Complex Multi-Dimensional Sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0b12a",
   "metadata": {},
   "source": [
    "### 4.1 **Advanced Batch Structures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b1049055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfp.distributions.Exponential(\"Exponential\", batch_shape=[2, 1, 2, 3], event_shape=[], dtype=float32)\n",
      "tfp.distributions.Independent(\"IndependentExponential\", batch_shape=[2, 1], event_shape=[2, 3], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create complex multi-dimensional batch structure\n",
    "rates = [\n",
    "    [[[1., 1.5, 0.8], [0.3, 0.4, 1.8]]],\n",
    "    [[[0.2, 0.4, 1.4], [0.4, 1.1, 0.9]]]\n",
    "]\n",
    "\n",
    "exp = tfd.Exponential(rate=rates)\n",
    "print(exp)\n",
    "# Output: batch_shape=[2, 1, 2, 3], event_shape=[]\n",
    "\n",
    "# Apply Independent with reinterpreted_batch_ndims=2\n",
    "ind_exp = tfd.Independent(exp, reinterpreted_batch_ndims=2)\n",
    "print(ind_exp)\n",
    "# Output: batch_shape=[2, 1], event_shape=[2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cb71fe",
   "metadata": {},
   "source": [
    "**Complex Shape Analysis**:\n",
    "- **Original**: `batch_shape=[2, 1, 2, 3], event_shape=[]` → 12 independent scalars\n",
    "- **After Independent**: `batch_shape=[2, 1], event_shape=[2, 3]` → 2×1 = 2 joint distributions over 2×3 matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b3ef0e",
   "metadata": {},
   "source": [
    "### 4.2 **Multi-Dimensional Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15fdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex samples shape: (4, 2, 2, 1, 2, 3)\n",
      "Complex samples structure:\n",
      "  Sample shape: [4, 2]\n",
      "  Batch shape: [2, 1]\n",
      "  Event shape: [2, 3]\n",
      "  Final shape: (4, 2, 2, 1, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Sample with specific sample shape\n",
    "rates = [\n",
    "    [[[1., 1.5, 0.8], [0.3, 0.4, 1.8]]],\n",
    "    [[[0.2, 0.4, 1.4], [0.4, 1.1, 0.9]]]\n",
    "]\n",
    "\n",
    "exp = tfd.Exponential(rate=rates)\n",
    "ind_exp = tfd.Independent(exp, reinterpreted_batch_ndims=2)\n",
    "\n",
    "# Sample with shape [4, 2]\n",
    "complex_samples = ind_exp.sample([4, 2])\n",
    "print(\"Complex samples shape:\", complex_samples.shape)  \n",
    "print(\"Complex samples structure:\")\n",
    "print(\"  Sample shape: [4, 2]\")\n",
    "print(\"  Batch shape: [2, 1]\") \n",
    "print(\"  Event shape: [2, 3]\")\n",
    "print(\"  Final shape: (4, 2, 2, 1, 2, 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f2fd8",
   "metadata": {},
   "source": [
    "## 5. **Log Probability Computations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588a897",
   "metadata": {},
   "source": [
    "### 5.1 **Basic Log Probability Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243a4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar log prob shape: (2, 1)\n",
      "Scalar log prob values: tf.Tensor(\n",
      "[[-4.2501554]\n",
      " [-5.3155975]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create the same distribution structure\n",
    "rates = [\n",
    "    [[[1., 1.5, 0.8], [0.3, 0.4, 1.8]]],\n",
    "    [[[0.2, 0.4, 1.4], [0.4, 1.1, 0.9]]]\n",
    "]\n",
    "\n",
    "exp = tfd.Exponential(rate=rates)\n",
    "ind_exp = tfd.Independent(exp, reinterpreted_batch_ndims=2)\n",
    "\n",
    "# Single scalar input (broadcasts across all distributions)\n",
    "scalar_log_prob = ind_exp.log_prob(0.5)\n",
    "print(\"Scalar log prob shape:\", scalar_log_prob.shape)  \n",
    "print(\"Scalar log prob values:\", scalar_log_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6851c326",
   "metadata": {},
   "source": [
    "**Broadcasting Behavior**:\n",
    "- **Input**: `0.5` (scalar)\n",
    "- **Distribution event_shape**: `[2, 3]` \n",
    "- **Broadcasting**: Scalar expands to `[2, 3]` filled with `0.5`\n",
    "- **Output**: `(2, 1)` - one log probability per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c2548",
   "metadata": {},
   "source": [
    "### 5.2 **Vector Input Log Probability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6596133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector log prob shape: (2, 1)\n",
      "Vector log prob values: tf.Tensor(\n",
      "[[-4.7701554]\n",
      " [-5.8855977]], shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Vector input matching event shape\n",
    "rates = [\n",
    "    [[[1., 1.5, 0.8], [0.3, 0.4, 1.8]]],\n",
    "    [[[0.2, 0.4, 1.4], [0.4, 1.1, 0.9]]]\n",
    "]\n",
    "\n",
    "exp = tfd.Exponential(rate=rates)\n",
    "ind_exp = tfd.Independent(exp, reinterpreted_batch_ndims=2)\n",
    "\n",
    "# Input shape (1, 3) - partially matches event_shape \n",
    "vector_input = [[0.3, 0.5, 0.8]]  # Shape: (1, 3)\n",
    "vector_log_prob = ind_exp.log_prob(vector_input)\n",
    "print(\"Vector log prob shape:\", vector_log_prob.shape)  \n",
    "print(\"Vector log prob values:\", vector_log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66f8b6a",
   "metadata": {},
   "source": [
    "## 6. **Advanced Broadcasting & Shape Mechanics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eaf4b5",
   "metadata": {},
   "source": [
    "### 6.1 **Complex Broadcasting Patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "99b4d621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex input shape: (5, 1, 1, 2, 1)\n",
      "Complex log prob shape: (5, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "tfd = tfp.distributions\n",
    "\n",
    "# Create complex distribution structure\n",
    "rates = [\n",
    "    [[[1., 1.5, 0.8], [0.3, 0.4, 1.8]]],\n",
    "    [[[0.2, 0.4, 1.4], [0.4, 1.1, 0.9]]]\n",
    "]\n",
    "\n",
    "exp = tfd.Exponential(rate=rates)\n",
    "ind_exp = tfd.Independent(exp, reinterpreted_batch_ndims=2)\n",
    "\n",
    "# Complex input with shape\n",
    "complex_input = tf.random.uniform((5, 1, 1, 2, 1))\n",
    "print(\"Complex input shape:\", complex_input.shape)  \n",
    "\n",
    "complex_log_prob = ind_exp.log_prob(complex_input)\n",
    "print(\"Complex log prob shape:\", complex_log_prob.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5efdc2",
   "metadata": {},
   "source": [
    "**Advanced Broadcasting Rules**:\n",
    "- **Distribution**: `batch_shape=[2, 1], event_shape=[2, 3]`\n",
    "- **Input**: `(5, 1, 1, 2, 1)`\n",
    "- **Broadcasting**: Input shape aligns with batch + event dimensions\n",
    "- **Output**: Broadcasted result based on TensorFlow broadcasting rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af0960",
   "metadata": {},
   "source": [
    "### 6.2 **Multivariate Distribution with Independent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c548840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base distribution:\n",
      "  batch_shape: (2, 3)\n",
      "  event_shape: (4,)\n",
      "After Independent:\n",
      "  batch_shape: (2,)\n",
      "  event_shape: (3, 4)\n",
      "Result shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "tfd = tfp.distributions\n",
    "\n",
    "# Create multivariate distribution with Independent wrapper\n",
    "loc = tf.zeros((2, 3, 1))          # Shape: (2, 3, 1)\n",
    "scale_diag = tf.ones(4)            # Shape: (4,)\n",
    "\n",
    "# Create base multivariate normal\n",
    "base_mv_normal = tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale_diag)\n",
    "print(\"Base distribution:\")\n",
    "print(f\"  batch_shape: {base_mv_normal.batch_shape}\")  \n",
    "print(f\"  event_shape: {base_mv_normal.event_shape}\") \n",
    "\n",
    "# Wrap with Independent\n",
    "dist = tfd.Independent(base_mv_normal)  # Default reinterpreted_batch_ndims=1\n",
    "print(\"After Independent:\")\n",
    "print(f\"  batch_shape: {dist.batch_shape}\")  \n",
    "print(f\"  event_shape: {dist.event_shape}\")  \n",
    "\n",
    "# Log probability evaluation\n",
    "test_input = tf.random.uniform((2, 1, 1, 4))\n",
    "result_log_prob = dist.log_prob(test_input)\n",
    "print(\"Result shape:\", result_log_prob.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a069fd",
   "metadata": {},
   "source": [
    "**Multivariate + Independent Analysis**:\n",
    "- **Base**: MultivariateNormal with `batch_shape=[2, 3, 1], event_shape=`[5]\n",
    "- **Independent**: Moves last batch dimension to event: `batch_shape=[2, 3], event_shape=[1, 4]`\n",
    "- **Result**: Joint log probability over `[1, 4]` dimensional events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53769d",
   "metadata": {},
   "source": [
    "## 7. **Professional Patterns & Best Practices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb01912",
   "metadata": {},
   "source": [
    "### 7.1 **Efficient Sampling Strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9d60eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples shape: (5000, 10)\n",
      "Log probs shape: (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "def efficient_monte_carlo_estimation(distribution, n_samples=1000, chunk_size=100):\n",
    "    \"\"\"\n",
    "    Memory-efficient Monte Carlo sampling for large n_samples\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    log_probs = []\n",
    "    \n",
    "    for i in range(0, n_samples, chunk_size):\n",
    "        current_chunk = min(chunk_size, n_samples - i)\n",
    "        \n",
    "        # Sample and evaluate in chunks to manage memory\n",
    "        chunk_samples = distribution.sample(current_chunk)\n",
    "        chunk_log_probs = distribution.log_prob(chunk_samples)\n",
    "        \n",
    "        samples.append(chunk_samples)\n",
    "        log_probs.append(chunk_log_probs)\n",
    "    \n",
    "    return tf.concat(samples, axis=0), tf.concat(log_probs, axis=0)\n",
    "\n",
    "# Example usage\n",
    "dist = tfd.Independent(tfd.Normal(loc=tf.zeros(10), scale=tf.ones(10)))\n",
    "samples, log_probs = efficient_monte_carlo_estimation(dist, n_samples=5000)\n",
    "print(f\"Samples shape: {samples.shape}\")    \n",
    "print(f\"Log probs shape: {log_probs.shape}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0caea",
   "metadata": {},
   "source": [
    "### 7.2 **Broadcasting Validation Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e2a1704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LOG PROB INPUT VALIDATION ===\n",
      "Distribution batch_shape: (3,)\n",
      "Distribution event_shape: (4,)\n",
      "Input tensor shape: (2, 1, 4)\n",
      "✅ Event shape compatible\n",
      "✅ Expected output shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "def validate_log_prob_input_shape(distribution, input_tensor):\n",
    "    \"\"\"\n",
    "    Validate that input tensor can be evaluated by distribution.log_prob()\n",
    "    \"\"\"\n",
    "    batch_shape = distribution.batch_shape\n",
    "    event_shape = distribution.event_shape\n",
    "    input_shape = input_tensor.shape\n",
    "    \n",
    "    print(\"=== LOG PROB INPUT VALIDATION ===\")\n",
    "    print(f\"Distribution batch_shape: {batch_shape}\")\n",
    "    print(f\"Distribution event_shape: {event_shape}\")\n",
    "    print(f\"Input tensor shape: {input_shape}\")\n",
    "    \n",
    "    # Check if input is compatible with event shape\n",
    "    event_ndims = len(event_shape)\n",
    "    if len(input_shape) < event_ndims:\n",
    "        print(\"❌ ERROR: Input rank too small for event shape\")\n",
    "        return False\n",
    "    \n",
    "    input_event_shape = input_shape[-event_ndims:] if event_ndims > 0 else []\n",
    "    \n",
    "    # Check event shape compatibility\n",
    "    try:\n",
    "        tf.broadcast_static_shape(input_event_shape, event_shape)\n",
    "        print(\"✅ Event shape compatible\")\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"❌ ERROR: Input event shape incompatible\")\n",
    "        return False\n",
    "    \n",
    "    # Predict output shape\n",
    "    try:\n",
    "        sample_and_batch_shape = input_shape[:-event_ndims] if event_ndims > 0 else input_shape\n",
    "        output_shape = tf.broadcast_static_shape(sample_and_batch_shape, batch_shape)\n",
    "        print(f\"✅ Expected output shape: {output_shape}\")\n",
    "        return True\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        print(\"❌ ERROR: Batch broadcasting will fail\")\n",
    "        return False\n",
    "\n",
    "# Example validation\n",
    "dist = tfd.Independent(tfd.Normal(loc=tf.zeros((3, 4)), scale=tf.ones((3, 4))))\n",
    "input_tensor = tf.random.normal((2, 1, 4))\n",
    "is_valid = validate_log_prob_input_shape(dist, input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebb1b5",
   "metadata": {},
   "source": [
    "### 7.3 **Probabilistic Training Loss Patterns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ebb474b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Custom loss for probabilistic outputs using log_prob\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction=tf.keras.losses.Reduction.AUTO, name='probabilistic_loss'):\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "    \n",
    "    def call(self, y_true, y_pred_distribution):\n",
    "        \"\"\"\n",
    "        y_true: Ground truth values\n",
    "        y_pred_distribution: TFP Distribution object\n",
    "        \"\"\"\n",
    "        # Negative log likelihood\n",
    "        nll = -y_pred_distribution.log_prob(y_true)\n",
    "        return nll\n",
    "\n",
    "# Example usage in training\n",
    "@tf.function\n",
    "def train_step_with_sampling(features, targets, model, optimizer):\n",
    "    \"\"\"\n",
    "    Training step that uses both sampling and log_prob\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Model outputs distribution\n",
    "        pred_distribution = model(features, training=True)\n",
    "        \n",
    "        # Main loss: negative log likelihood\n",
    "        nll_loss = -tf.reduce_mean(pred_distribution.log_prob(targets))\n",
    "        \n",
    "        # Regularization using samples\n",
    "        samples = pred_distribution.sample(5)\n",
    "        sample_std = tf.math.reduce_std(samples, axis=0)\n",
    "        regularization = 0.01 * tf.reduce_mean(sample_std)  # Encourage diversity\n",
    "        \n",
    "        total_loss = nll_loss + regularization\n",
    "    \n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss,\n",
    "        'nll_loss': nll_loss,\n",
    "        'regularization': regularization\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36499fc",
   "metadata": {},
   "source": [
    "## 8. **Expert Applications**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0bbde3",
   "metadata": {},
   "source": [
    "### 8.1 **Monte Carlo Variational Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2510d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_elbo(encoder, decoder, data, n_samples=10):\n",
    "    \"\"\"\n",
    "    Compute ELBO using Monte Carlo estimation with sampling and log_prob\n",
    "    \"\"\"\n",
    "    # Encode to get posterior distribution\n",
    "    posterior = encoder(data)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    z_samples = posterior.sample(n_samples)  # Shape: (n_samples, batch_size, latent_dim)\n",
    "    \n",
    "    # Decode samples\n",
    "    reconstruction_logits = decoder(z_samples)\n",
    "    \n",
    "    # Prior distribution\n",
    "    prior = tfd.Independent(tfd.Normal(loc=0., scale=1.), reinterpreted_batch_ndims=1)\n",
    "    \n",
    "    # Compute ELBO components\n",
    "    # 1. Reconstruction term: E_q[log p(x|z)]\n",
    "    reconstruction_dist = tfd.Independent(\n",
    "        tfd.Bernoulli(logits=reconstruction_logits), \n",
    "        reinterpreted_batch_ndims=1\n",
    "    )\n",
    "    reconstruction_term = tf.reduce_mean(reconstruction_dist.log_prob(data))\n",
    "    \n",
    "    # 2. KL divergence: E_q[log q(z|x) - log p(z)]\n",
    "    posterior_log_prob = tf.reduce_mean(posterior.log_prob(z_samples))\n",
    "    prior_log_prob = tf.reduce_mean(prior.log_prob(z_samples))\n",
    "    kl_divergence = posterior_log_prob - prior_log_prob\n",
    "    \n",
    "    # ELBO = Reconstruction - KL\n",
    "    elbo = reconstruction_term - kl_divergence\n",
    "    \n",
    "    return {\n",
    "        'elbo': elbo,\n",
    "        'reconstruction': reconstruction_term,\n",
    "        'kl_divergence': kl_divergence\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e77c1",
   "metadata": {},
   "source": [
    "### 8.2 **Advanced Sampling Techniques**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39a86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated E[X^2]: 26.391807556152344\n",
      "Effective sample size: 568.8297119140625\n"
     ]
    }
   ],
   "source": [
    "class ImportanceSampler:\n",
    "    \"\"\"\n",
    "    Importance sampling using proposal distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, target_distribution, proposal_distribution):\n",
    "        self.target = target_distribution\n",
    "        self.proposal = proposal_distribution\n",
    "    \n",
    "    def sample(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generate importance-weighted samples\n",
    "        \"\"\"\n",
    "        # Sample from proposal\n",
    "        samples = self.proposal.sample(n_samples)\n",
    "        \n",
    "        # Compute importance weights\n",
    "        target_log_prob = self.target.log_prob(samples)\n",
    "        proposal_log_prob = self.proposal.log_prob(samples)\n",
    "        log_weights = target_log_prob - proposal_log_prob\n",
    "        \n",
    "        # Normalize weights\n",
    "        weights = tf.nn.softmax(log_weights)\n",
    "        \n",
    "        return {\n",
    "            'samples': samples,\n",
    "            'weights': weights,\n",
    "            'log_weights': log_weights,\n",
    "            'effective_sample_size': 1.0 / tf.reduce_sum(weights**2)\n",
    "        }\n",
    "    \n",
    "    def estimate_expectation(self, function, n_samples):\n",
    "        \"\"\"\n",
    "        Estimate E_target[function(X)] using importance sampling\n",
    "        \"\"\"\n",
    "        result = self.sample(n_samples)\n",
    "        function_values = function(result['samples'])\n",
    "        \n",
    "        # Weighted average\n",
    "        expectation = tf.reduce_sum(result['weights'] * function_values)\n",
    "        \n",
    "        return expectation, result['effective_sample_size']\n",
    "\n",
    "# Example usage\n",
    "target = tfd.Normal(loc=5., scale=1.)\n",
    "proposal = tfd.Normal(loc=4., scale=2.)\n",
    "sampler = ImportanceSampler(target, proposal)\n",
    "\n",
    "# Estimate E[X^2] under target distribution\n",
    "def square_function(x):\n",
    "    return x**2\n",
    "\n",
    "expectation, ess = sampler.estimate_expectation(square_function, n_samples=1000)\n",
    "print(f\"Estimated E[X^2]: {expectation}\")\n",
    "print(f\"Effective sample size: {ess}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ab65c",
   "metadata": {},
   "source": [
    "## 9. **Complete Reference Guide**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ef38a0",
   "metadata": {},
   "source": [
    "### 9.1 **Sampling Operations Reference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44470178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single sample: -0.5220753\n",
      "Batch samples: [-0.1365969  -1.218801   -0.9215488   2.7218962   1.5125786   1.4108888\n",
      "  1.6950028   1.4028981   0.7990797  -0.65465575  1.0263169   0.8830985\n",
      "  0.04019995 -1.0114911   0.25672132 -0.0426529   0.17955922 -0.99527174\n",
      " -0.601428    1.4637355   1.6804155  -0.70276546 -0.9713734  -1.0440786\n",
      " -0.86533374  0.02161358  0.7340682   2.1229079   0.7858814  -0.5298925\n",
      " -1.0749215   1.1408054   0.3555776  -0.08745395  0.00865405 -1.32541\n",
      "  0.39447325  0.56572866  0.25718474  0.77076024  0.8543685  -1.1066352\n",
      " -0.62045103  0.98014045 -0.30651346  1.415835   -1.933607    0.85357845\n",
      "  1.3835907   0.25166965 -1.2337768   1.040264   -2.7738826  -0.6555695\n",
      "  2.380446   -1.4398961  -0.21188603 -0.09578023  0.5760158   0.41028446\n",
      " -0.93355393  1.5430417  -2.0077636   0.45634276 -1.5455538  -0.21137801\n",
      "  0.60432875  1.9447913  -1.3108555  -1.3553311  -1.6904684   0.6628786\n",
      "  0.64199805  0.30658808  2.322921   -1.2522532   0.9507786   1.1831026\n",
      "  1.0265836  -0.67858493 -1.2120677  -0.6385424  -0.93031543  1.1304743\n",
      "  0.5717989  -0.31339905 -0.992359   -0.66067636  0.05468331 -0.02239556\n",
      " -0.13421923  0.08097219 -0.8455511   1.2402557  -1.0119678  -0.08862612\n",
      "  0.41137403 -0.29498816  0.03510866  0.6051137 ]\n",
      "Seeded samples: [ 1.3148774  -0.15421568  0.9113878  -0.7991441  -0.10875293  0.28436786\n",
      "  0.7661625  -0.6211289   0.9974318   0.21959257 -1.0798668  -1.9874831\n",
      "  1.5603696  -0.16909476 -1.3780487  -0.29801553  0.8410001   0.9225498\n",
      " -0.48379228 -1.6971248   0.09536484 -0.10675382 -1.7839274   1.7145915\n",
      " -1.1761415   1.1488945   0.4505377  -0.81724054  0.95193976  1.3913034\n",
      "  0.15862282  0.10488074  1.1451166  -1.0479503   0.21458012  0.25750676\n",
      " -0.99603957  0.55202    -0.8870138   3.4017045   0.34291962 -0.4389471\n",
      " -1.5277438  -0.8316435   0.3434062  -2.7989998  -0.9340011  -0.40604064\n",
      "  1.2880986   1.2897398  -0.3741515  -1.4218495  -0.13424467  1.0253482\n",
      " -3.2489176   1.8755275   0.8539876   0.6199211  -0.21415219  2.0731623\n",
      "  0.19485866 -0.36870828  0.32352057  1.7784952   0.67288345  1.192604\n",
      "  0.02937807  1.3695234  -0.84422374  0.14643075 -2.167099   -0.54043925\n",
      " -1.3511175  -1.1596807  -0.27452996 -0.7361402   0.38832134 -0.24853493\n",
      " -0.79855597 -0.46657825  1.1286335  -1.1491549   0.2331937  -0.75925595\n",
      "  0.38633636 -1.4149377  -1.7656256  -0.665978   -0.5536773  -1.4172283\n",
      "  1.1051842   1.2187223  -0.6621953   0.7448888  -1.1173725   0.71882343\n",
      "  0.5128387  -0.7941299   1.241065   -0.8387284 ]\n"
     ]
    }
   ],
   "source": [
    "# === BASIC SAMPLING ===\n",
    "dist = tfd.Normal(loc=0., scale=1.)\n",
    "single = dist.sample()                          # Shape: ()\n",
    "batch = dist.sample(100)                        # Shape: (100,)\n",
    "seeded = dist.sample(100, seed=42)              # Reproducible\n",
    "\n",
    "print(\"Single sample:\", single.numpy())\n",
    "print(\"Batch samples:\", batch.numpy())\n",
    "print(\"Seeded samples:\", seeded.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "104f2bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch samples shape: (50, 2)\n"
     ]
    }
   ],
   "source": [
    "# === BATCH SAMPLING ===\n",
    "batch_dist = tfd.Normal(loc=[0., 1.], scale=[1., 2.])\n",
    "batch_samples = batch_dist.sample(50)           # Shape: (50, 2)\n",
    "\n",
    "print(\"Batch samples shape:\", batch_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2add0849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate samples shape: (30, 2)\n"
     ]
    }
   ],
   "source": [
    "# === MULTIVARIATE SAMPLING ===\n",
    "mv_dist = tfd.MultivariateNormalDiag(loc=[0., 1.], scale_diag=[1., 2.])\n",
    "mv_samples = mv_dist.sample(30)                 # Shape: (30, 2)\n",
    "print(\"Multivariate samples shape:\", mv_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "752aab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate samples shape: (30, 2)\n"
     ]
    }
   ],
   "source": [
    "# === INDEPENDENT SAMPLING ===\n",
    "indep_dist = tfd.Independent(tfd.Normal(loc=tf.zeros(5), scale=tf.ones(5)))\n",
    "indep_samples = indep_dist.sample(20)           # Shape: (20, 5)\n",
    "print(\"Multivariate samples shape:\", mv_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "efa31f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complex samples shape: (10, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "# === COMPLEX SAMPLING ===\n",
    "complex_samples = dist.sample([10, 3, 2])       # Shape: (10, 3, 2) + batch_shape + event_shape\n",
    "print(\"Complex samples shape:\", complex_samples.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df872d51",
   "metadata": {},
   "source": [
    "### 9.2 **Log Probability Operations Reference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5745086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-0.9189385, shape=(), dtype=float32)\n",
      "tf.Tensor([-0.9189385 -1.4189385 -2.9189386], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Creating distribution\n",
    "dist = tfd.Normal(loc=0., scale=1.)\n",
    "print(dist.log_prob(0.))                        # Scalar input\n",
    "print(dist.log_prob([0., 1., 2.]))              # Vector input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06869936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar log prob: -1.0439385\n",
      "Vector log prob: [-0.9189385 -1.4189385 -2.9189386]\n"
     ]
    }
   ],
   "source": [
    "# === BASIC LOG PROB ===\n",
    "scalar_log_prob = dist.log_prob(0.5)            # Shape: ()\n",
    "vector_log_prob = dist.log_prob([0., 1., 2.])   # Shape: (3,)\n",
    "\n",
    "print(\"Scalar log prob:\", scalar_log_prob.numpy())\n",
    "print(\"Vector log prob:\", vector_log_prob.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f78aca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch log prob: [-1.0439385 -1.6433357]\n"
     ]
    }
   ],
   "source": [
    "# === BATCH LOG PROB ===\n",
    "batch_dist = tfd.Normal(loc=[0., 1.], scale=[1., 2.])\n",
    "batch_log_prob = batch_dist.log_prob([0.5, 1.5])  \n",
    "\n",
    "print(\"Batch log prob:\", batch_log_prob.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fbe518e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate log prob: -2.6872742\n"
     ]
    }
   ],
   "source": [
    "# === MULTIVARIATE LOG PROB ===\n",
    "mv_dist = tfd.MultivariateNormalDiag(loc=[0., 1.], scale_diag=[1., 2.])\n",
    "mv_log_prob = mv_dist.log_prob([0.5, 1.5])      # Shape: () - joint probability\n",
    "\n",
    "print(\"Multivariate log prob:\", mv_log_prob.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7dd3ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent log prob: [-0.9189385 -1.4189385 -2.9189386]\n"
     ]
    }
   ],
   "source": [
    "# === INDEPENDENT LOG PROB ===\n",
    "indep_dist = tfd.Independent(tfd.Normal(loc=tf.zeros(3), scale=tf.ones(3)))\n",
    "indep_log_prob = indep_dist.log_prob([0., 1., 2.])  # Shape: () - sum of components\n",
    "print(\"Independent log prob:\", indep_log_prob.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7e9f13d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Broadcast log prob shape: (5, 3, 2)\n"
     ]
    }
   ],
   "source": [
    "# === BROADCASTING LOG PROB ===\n",
    "broadcast_log_prob = dist.log_prob(tf.ones((5, 3, 2)))  # Broadcasting rules apply\n",
    "print(\"Broadcast log prob shape:\", broadcast_log_prob.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d1017",
   "metadata": {},
   "source": [
    "### 9.3 **Shape Rules for Sampling and Log Prob**\n",
    "\n",
    "| **Operation** | **Distribution Shape** | **Input/Sample Shape** | **Output Shape** |\n",
    "|---------------|------------------------|------------------------|------------------|\n",
    "| **`sample()`** | `batch=[B], event=[E]` | `sample=[S]` | `[S] + [B] + [E]` |\n",
    "| **`sample(n)`** | `batch=[B], event=[E]` | `n` (scalar) | `[n] + [B] + [E]` |\n",
    "| **`log_prob(x)`** | `batch=[B], event=[E]` | `x.shape` | `broadcast([x.shape[:-len(E)]], [B])` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e8478d",
   "metadata": {},
   "source": [
    "## 💡 **Final Notes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af227646",
   "metadata": {},
   "source": [
    "- **The Sampling-Inference Cycle**: Modern probabilistic ML alternates between **sampling** (generating data, exploring parameter space) and **log_prob evaluation** (measuring likelihood, computing gradients). Master both to unlock the full power of probabilistic modeling.\n",
    "\n",
    "- **Memory vs Accuracy Trade-off**: Large sample sizes give better Monte Carlo estimates but consume more memory. Use chunked sampling for large-scale problems.\n",
    "\n",
    "- **Log-Space Numerics**: Always use `log_prob()` instead of `prob()` for numerical stability. Probabilities can underflow, but log probabilities remain stable.\n",
    "\n",
    "- **Shape Debugging Strategy**: When shapes don't work, trace through: `sample_shape + batch_shape + event_shape` for sampling, and broadcasting rules for log_prob inputs.\n",
    "\n",
    "- **Broadcasting is Your Friend**: TFP's broadcasting enables vectorized operations across complex hierarchies. Learn it well—it's essential for efficient probabilistic computation.\n",
    "\n",
    "- **Gradient Flow**: `log_prob()` is differentiable through its inputs and distribution parameters. This enables gradient-based optimization of probabilistic models—the foundation of modern deep learning.\n",
    "\n",
    "- Sampling and log probabilities are the **computational engines** of probabilistic machine learning. Every advanced technique—from VAEs to Bayesian neural networks to MCMC—builds on these fundamental operations. Master them, and you unlock the full potential of uncertainty quantification and probabilistic reasoning! 🚀\n",
    "\n",
    "\n",
    "> Once comfortable with sampling and log probabilities, explore bijectors for distribution transformations, custom training loops with `tf.GradientTape`, and advanced inference techniques like Hamiltonian Monte Carlo to build cutting-edge probabilistic systems.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Probabilistic-Deep-Learning-with-TensorFlow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
