{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v2aM6Wa1v_A"
      },
      "source": [
        "# **Trainable Distributions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìë Table of Contents\n",
        "\n",
        "- **1. [Foundation - What Are Trainable Distributions?](#1--foundation---what-are-trainable-distributions)**\n",
        "    - 1.1 [The Big Picture](#11-the-big-picture)\n",
        "\n",
        "- **2. [Setup & Your First Trainable Distribution](#2--setup--your-first-trainable-distribution)**\n",
        "    - 2.1 [Essential Imports & Setup](#21-essential-imports--setup)  \n",
        "    - 2.2 [Creating Your First Trainable Distribution](#22-creating-your-first-trainable-distribution)\n",
        "\n",
        "- **3. [Training Trainable Distributions](#3--training-trainable-distributions)**\n",
        "    - 3.1 [Complete Training Example - Learning from Data](#31-complete-training-example---learning-from-data)  \n",
        "    - 3.2 [Loss Function and Training Setup](#32-loss-function-and-training-setup)  \n",
        "    - 3.3 [Training Loop with Progress Tracking](#33-training-loop-with-progress-tracking)\n",
        "\n",
        "- **4. [Advanced Trainable Distribution Patterns](#4--advanced-trainable-distribution-patterns)**\n",
        "    - 4.1 [Multiple Trainable Parameters](#41-multiple-trainable-parameters)  \n",
        "    - 4.2 [Trainable Multivariate Distributions](#42-trainable-multivariate-distributions)\n",
        "\n",
        "- **5. [Probabilistic Neural Network Integration](#5-Ô∏è-probabilistic-neural-network-integration)**\n",
        "    - 5.1 [Trainable Distribution Layers](#51-trainable-distribution-layers)  \n",
        "    - 5.2 [Custom Training with Uncertainty](#52-custom-training-with-uncertainty)\n",
        "\n",
        "- **6. [Expert Applications](#6--expert-applications)**\n",
        "    - 6.1 [Bayesian Neural Networks with Trainable Priors](#61-bayesian-neural-networks-with-trainable-priors)  \n",
        "    - 6.2 [Generative Models with Trainable Distributions](#62-generative-models-with-trainable-distributions)\n",
        "\n",
        "- **7. [Complete Reference Guide](#7--complete-reference-guide)**\n",
        "    - 7.1 [Trainable Distribution Creation Patterns](#71-trainable-distribution-creation-patterns)  \n",
        "    - 7.2 [Training Patterns Reference](#72-training-patterns-reference)  \n",
        "    - 7.3 [Common Parameter Constraints](#73-common-parameter-constraints)\n",
        "\n",
        "- **[Final Notes](#final-notes)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. **Foundation - What Are Trainable Distributions?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.1 **The Big Picture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trainable distributions are TensorFlow Probability distributions whose **parameters can be learned through gradient-based optimization**. Instead of fixed parameters, these distributions have parameters represented as `tf.Variable` objects that can be updated during training, enabling the model to learn the underlying data distribution automatically.\n",
        "\n",
        "**Key Insight**: Trainable distributions transform static probability models into **dynamic, learnable components** that can be integrated seamlessly into neural networks and optimized using standard deep learning techniques like backpropagation.\n",
        "\n",
        "**The Core Transformation**:\n",
        "- **Static**: `tfd.Normal(loc=0., scale=1.)` ‚Üí Fixed parameters\n",
        "- **Trainable**: `tfd.Normal(loc=tf.Variable(0.), scale=tf.Variable(1.))` ‚Üí Learnable parameters\n",
        "\n",
        "**Why Revolutionary**: This enables **probabilistic deep learning** where uncertainty quantification, generative modeling, and Bayesian inference become end-to-end differentiable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. **Setup & Your First Trainable Distribution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 **Essential Imports & Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "\n",
        "# Standard alias for distributions\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 **Creating Your First Trainable Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable variables: (<tf.Variable 'loc:0' shape=() dtype=float32, numpy=0.0>,)\n",
            "Distribution: tfp.distributions.Normal(\"Normal\", batch_shape=[], event_shape=[], dtype=float32)\n",
            "Current location parameter: 0.0\n",
            "Fixed scale parameter: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Create trainable Normal distribution\n",
        "normal = tfd.Normal(loc=tf.Variable(0., name='loc'), scale=1.)\n",
        "\n",
        "# Check trainable variables\n",
        "print(\"Trainable variables:\", normal.trainable_variables)\n",
        "# Output: (<tf.Variable 'loc:0' shape=() dtype=float32, numpy=0.0>,)\n",
        "\n",
        "print(\"Distribution:\", normal)\n",
        "print(\"Current location parameter:\", normal.loc.numpy())  # 0.0\n",
        "print(\"Fixed scale parameter:\", normal.scale.numpy())     # 1.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key Components**:\n",
        "- **`tf.Variable(0., name='loc')`**: Trainable location parameter, initialized to 0\n",
        "- **`scale=1.`**: Fixed scale parameter (not trainable)\n",
        "- **`normal.trainable_variables`**: Tuple containing all trainable parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. **Training Trainable Distributions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 **Complete Training Example - Learning from Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample data statistics:\n",
            "  True mean: 2.0290\n",
            "  True std: 1.4681\n",
            "  Sample size: 1000\n",
            "Initial location parameter: 0.0000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import numpy as np\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "# Create some sample data (you can replace this with your actual data)\n",
        "np.random.seed(42)  # For reproducibility\n",
        "x_samples = tf.constant(np.random.normal(2.0, 1.5, 1000), dtype=tf.float32)\n",
        "\n",
        "print(f\"Sample data statistics:\")\n",
        "print(f\"  True mean: {tf.reduce_mean(x_samples).numpy():.4f}\")\n",
        "print(f\"  True std: {tf.math.reduce_std(x_samples).numpy():.4f}\")\n",
        "print(f\"  Sample size: {len(x_samples)}\")\n",
        "\n",
        "# Define number of training steps\n",
        "num_steps = 1000\n",
        "\n",
        "# Create trainable distribution\n",
        "normal = tfd.Normal(loc=tf.Variable(0., name='loc'), scale=1.)\n",
        "print(f\"Initial location parameter: {normal.loc.numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 **Loss Function and Training Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Define negative log likelihood (NLL) loss function\n",
        "def nll(x_train):  # Defining the negative log likelihood(nll)\n",
        "    \"\"\"\n",
        "    Compute negative log-likelihood of data under current distribution\n",
        "    \"\"\"\n",
        "    log_likelihood = normal.log_prob(x_train)  # Shape: (1000,)\n",
        "    mean_log_likelihood = tf.reduce_mean(log_likelihood)\n",
        "    return -mean_log_likelihood  # Negative for minimization\n",
        "\n",
        "\"\"\"\n",
        "This function get_loss_and_grads takes a batch of training examples as an input\n",
        "and computes the loss and gradients for our model.\n",
        "\"\"\"\n",
        "@tf.function  # Compile for performance\n",
        "def get_loss_and_grads(x_train):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(normal.trainable_variables)\n",
        "        loss = nll(x_train)\n",
        "    grads = tape.gradient(loss, normal.trainable_variables)\n",
        "    return loss, grads\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
        "print(\"Training setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 **Training Loop with Progress Tracking**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n",
            "==================================================\n",
            "Step    0: Loss = 4.0550, Estimated loc = 0.1014\n",
            "Step  100: Loss = 1.9967, Estimated loc = 2.0176\n",
            "Step  200: Loss = 1.9966, Estimated loc = 2.0289\n",
            "Step  300: Loss = 1.9966, Estimated loc = 2.0290\n",
            "Step  400: Loss = 1.9966, Estimated loc = 2.0290\n",
            "Step  500: Loss = 1.9966, Estimated loc = 2.0290\n",
            "Step  600: Loss = 1.9966, Estimated loc = 2.0290\n",
            "Step  700: Loss = 1.9966, Estimated loc = 2.0290\n",
            "Step  800: Loss = 1.9966, Estimated loc = 2.0290\n",
            "Step  900: Loss = 1.9966, Estimated loc = 2.0290\n",
            "==================================================\n",
            "\n",
            "Training completed!\n",
            "Final estimated location parameter: 2.0290\n",
            "True mean of sample data: 2.0290\n",
            "Estimation error: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Training loop with progress tracking\n",
        "print(\"\\nStarting training...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for step in range(num_steps):\n",
        "    loss, grads = get_loss_and_grads(x_samples)\n",
        "    optimizer.apply_gradients(zip(grads, normal.trainable_variables))\n",
        "    \n",
        "    # Print progress every 100 steps\n",
        "    if step % 100 == 0:\n",
        "        current_loc = normal.loc.numpy()\n",
        "        print(f\"Step {step:4d}: Loss = {loss:.4f}, Estimated loc = {current_loc:.4f}\")\n",
        "\n",
        "# Final results\n",
        "print(\"=\" * 50)\n",
        "print(f\"\\nTraining completed!\")\n",
        "print(f\"Final estimated location parameter: {normal.loc.numpy():.4f}\")\n",
        "print(f\"True mean of sample data: {tf.reduce_mean(x_samples).numpy():.4f}\")\n",
        "print(f\"Estimation error: {abs(normal.loc.numpy() - tf.reduce_mean(x_samples).numpy()):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. **Advanced Trainable Distribution Patterns**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 **Multiple Trainable Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable variables: 2\n",
            "Step 0: Loss = 4.0550, loc = 0.0100, scale = 1.0063\n",
            "Step 100: Loss = 2.1725, loc = 0.7820, scale = 1.4595\n",
            "Step 200: Loss = 1.9307, loc = 1.2481, scale = 1.5905\n",
            "Step 300: Loss = 1.8498, loc = 1.5779, scale = 1.5931\n",
            "Step 400: Loss = 1.8167, loc = 1.8013, scale = 1.5502\n",
            "\n",
            "Final parameters:\n",
            "  Learned loc: 1.9312\n",
            "  Learned scale: 1.5085\n",
            "  True std: 1.4681\n"
          ]
        }
      ],
      "source": [
        "class FullyTrainableNormal:\n",
        "    \"\"\"\n",
        "    Normal distribution with both loc and scale trainable\n",
        "    \"\"\"\n",
        "    def __init__(self, initial_loc=0., initial_scale=1.):\n",
        "        self.raw_loc = tf.Variable(initial_loc, name='raw_loc')\n",
        "        self.raw_scale = tf.Variable(\n",
        "            tf.math.log(tf.math.expm1(initial_scale)), \n",
        "            name='raw_scale'\n",
        "        )\n",
        "    \n",
        "    @property\n",
        "    def distribution(self):\n",
        "        # Ensure scale is always positive using softplus\n",
        "        loc = self.raw_loc\n",
        "        scale = tf.nn.softplus(self.raw_scale) + 1e-6\n",
        "        return tfd.Normal(loc=loc, scale=scale)\n",
        "    \n",
        "    @property\n",
        "    def trainable_variables(self):\n",
        "        return [self.raw_loc, self.raw_scale]\n",
        "    \n",
        "    def log_prob(self, x):\n",
        "        return self.distribution.log_prob(x)\n",
        "    \n",
        "    def sample(self, sample_shape=()):\n",
        "        return self.distribution.sample(sample_shape)\n",
        "\n",
        "# Usage example\n",
        "fully_trainable = FullyTrainableNormal(initial_loc=0., initial_scale=1.)\n",
        "print(\"Trainable variables:\", len(fully_trainable.trainable_variables))\n",
        "\n",
        "# Training both parameters\n",
        "def train_both_parameters(data, model, steps=500):\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step():\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = -tf.reduce_mean(model.log_prob(data))\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return loss\n",
        "    \n",
        "    for step in range(steps):\n",
        "        loss = train_step()\n",
        "        if step % 100 == 0:\n",
        "            current_dist = model.distribution\n",
        "            print(f\"Step {step}: Loss = {loss:.4f}, \"\n",
        "                  f\"loc = {current_dist.loc.numpy():.4f}, \"\n",
        "                  f\"scale = {current_dist.scale.numpy():.4f}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Train on the same sample data\n",
        "trained_model = train_both_parameters(x_samples, fully_trainable)\n",
        "final_dist = trained_model.distribution\n",
        "print(f\"\\nFinal parameters:\")\n",
        "print(f\"  Learned loc: {final_dist.loc.numpy():.4f}\")\n",
        "print(f\"  Learned scale: {final_dist.scale.numpy():.4f}\")\n",
        "print(f\"  True std: {tf.math.reduce_std(x_samples).numpy():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 **Trainable Multivariate Distributions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multivariate trainable variables: 1\n",
            "Variable shapes: [TensorShape([3])]\n",
            "\n",
            "Training multivariate distribution...\n",
            "Step 0: Loss = 10.0395\n",
            "Step 50: Loss = 7.3503\n",
            "Step 100: Loss = 5.9957\n",
            "Step 150: Loss = 5.3188\n",
            "Step 200: Loss = 4.9917\n",
            "Step 250: Loss = 4.8454\n",
            "\n",
            "Learned mean: [ 0.9956153  -0.46135303  1.8014729 ]\n",
            "True mean: [1.0, -0.5, 2.0]\n"
          ]
        }
      ],
      "source": [
        "def create_trainable_multivariate_normal(event_size, use_full_covariance=False):\n",
        "    \"\"\"\n",
        "    Create trainable multivariate normal distribution\n",
        "    \"\"\"\n",
        "    # Trainable location parameter\n",
        "    loc = tf.Variable(\n",
        "        tf.zeros(event_size), \n",
        "        name='mvn_loc'\n",
        "    )\n",
        "    \n",
        "    if use_full_covariance:\n",
        "        # Full covariance matrix using Cholesky decomposition\n",
        "        scale_tril_size = event_size * (event_size + 1) // 2\n",
        "        raw_scale_tril = tf.Variable(\n",
        "            tf.zeros(scale_tril_size),\n",
        "            name='raw_scale_tril'\n",
        "        )\n",
        "        \n",
        "        # Transform to valid lower triangular matrix\n",
        "        scale_tril = tfp.bijectors.FillScaleTriL()(raw_scale_tril)\n",
        "        distribution = tfd.MultivariateNormalTriL(\n",
        "            loc=loc, \n",
        "            scale_tril=scale_tril\n",
        "        )\n",
        "    else:\n",
        "        # Diagonal covariance (simpler)\n",
        "        raw_scale_diag = tf.Variable(\n",
        "            tf.zeros(event_size),\n",
        "            name='raw_scale_diag'\n",
        "        )\n",
        "        \n",
        "        scale_diag = tf.nn.softplus(raw_scale_diag) + 1e-6\n",
        "        distribution = tfd.MultivariateNormalDiag(\n",
        "            loc=loc,\n",
        "            scale_diag=scale_diag\n",
        "        )\n",
        "    \n",
        "    return distribution\n",
        "\n",
        "# Example usage\n",
        "mv_dist = create_trainable_multivariate_normal(event_size=3, use_full_covariance=False)\n",
        "print(\"Multivariate trainable variables:\", len(mv_dist.trainable_variables))\n",
        "print(\"Variable shapes:\", [var.shape for var in mv_dist.trainable_variables])\n",
        "\n",
        "# Generate multivariate training data\n",
        "true_mean = [1., -0.5, 2.]\n",
        "true_cov = [[1., 0.3, 0.1], [0.3, 0.8, -0.2], [0.1, -0.2, 1.2]]\n",
        "mv_data = tf.random.normal([500, 3]) @ tf.linalg.cholesky(true_cov) + true_mean\n",
        "\n",
        "# Train multivariate distribution\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "@tf.function\n",
        "def mv_train_step():\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = -tf.reduce_mean(mv_dist.log_prob(mv_data))\n",
        "    grads = tape.gradient(loss, mv_dist.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, mv_dist.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "print(\"\\nTraining multivariate distribution...\")\n",
        "for step in range(300):\n",
        "    loss = mv_train_step()\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss:.4f}\")\n",
        "\n",
        "print(f\"\\nLearned mean: {mv_dist.mean().numpy()}\")\n",
        "print(f\"True mean: {true_mean}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. **Probabilistic Neural Network Integration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 **Trainable Distribution Layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProbabilisticDense(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Dense layer that outputs a trainable distribution\n",
        "    \"\"\"\n",
        "    def __init__(self, units, distribution_type='normal', **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.distribution_type = distribution_type\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        if self.distribution_type == 'normal':\n",
        "            # Parameters for Normal distribution\n",
        "            self.loc_layer = tf.keras.layers.Dense(self.units, name='loc_layer')\n",
        "            self.scale_layer = tf.keras.layers.Dense(self.units, name='scale_layer')\n",
        "        elif self.distribution_type == 'bernoulli':\n",
        "            # Parameters for Bernoulli distribution\n",
        "            self.logits_layer = tf.keras.layers.Dense(self.units, name='logits_layer')\n",
        "        \n",
        "        super().build(input_shape)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        if self.distribution_type == 'normal':\n",
        "            loc = self.loc_layer(inputs)\n",
        "            raw_scale = self.scale_layer(inputs)\n",
        "            scale = tf.nn.softplus(raw_scale) + 1e-6\n",
        "            \n",
        "            # Create batched normal distribution\n",
        "            batched_normal = tfd.Normal(loc=loc, scale=scale)\n",
        "            # Convert to multivariate for joint operations\n",
        "            return tfd.Independent(batched_normal, reinterpreted_batch_ndims=1)\n",
        "            \n",
        "        elif self.distribution_type == 'bernoulli':\n",
        "            logits = self.logits_layer(inputs)\n",
        "            batched_bernoulli = tfd.Bernoulli(logits=logits)\n",
        "            return tfd.Independent(batched_bernoulli, reinterpreted_batch_ndims=1)\n",
        "\n",
        "# Example probabilistic model\n",
        "class ProbabilisticRegressor(tf.keras.Model):\n",
        "    def __init__(self, hidden_units=64, output_units=3):\n",
        "        super().__init__()\n",
        "        self.hidden1 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
        "        self.hidden2 = tf.keras.layers.Dense(hidden_units, activation='relu')\n",
        "        self.output_layer = ProbabilisticDense(output_units, distribution_type='normal')\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        x = self.hidden1(inputs)\n",
        "        x = self.hidden2(x)\n",
        "        return self.output_layer(x)  # Returns distribution, not point estimate\n",
        "\n",
        "# Create and use probabilistic model\n",
        "model = ProbabilisticRegressor(hidden_units=32, output_units=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 **Custom Training with Uncertainty**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training probabilistic regressor...\n",
            "Epoch  0: Total Loss = 9.2536, NLL = 9.2536\n",
            "Epoch 10: Total Loss = 5.9630, NLL = 5.9630\n",
            "Epoch 20: Total Loss = 4.5082, NLL = 4.5082\n",
            "Epoch 30: Total Loss = 3.7724, NLL = 3.7724\n",
            "Epoch 40: Total Loss = 3.3635, NLL = 3.3635\n",
            "Epoch 50: Total Loss = 3.1161, NLL = 3.1161\n",
            "Epoch 60: Total Loss = 2.9438, NLL = 2.9438\n",
            "Epoch 70: Total Loss = 2.8014, NLL = 2.8014\n",
            "Epoch 80: Total Loss = 2.6649, NLL = 2.6649\n",
            "Epoch 90: Total Loss = 2.5198, NLL = 2.5198\n",
            "\n",
            "Predictions with uncertainty:\n",
            "Mean predictions shape: (10, 2)\n",
            "Std predictions shape: (10, 2)\n",
            "Sample predictions:\n",
            "[[[ 0.82389325 -1.4043255 ]\n",
            "  [-0.96353745  0.6602427 ]\n",
            "  [ 1.7169242  -0.34885058]\n",
            "  [ 1.7561765  -0.45557353]\n",
            "  [-0.3937552   2.03724   ]\n",
            "  [ 0.3635568  -0.01658844]\n",
            "  [-0.39817008 -0.04280734]\n",
            "  [-0.99177986 -0.29655802]\n",
            "  [ 1.1118047  -1.3519115 ]\n",
            "  [ 2.4875593  -1.9801378 ]]\n",
            "\n",
            " [[ 2.3026707   0.6278113 ]\n",
            "  [ 0.03576642 -1.5997378 ]\n",
            "  [ 0.31301594  0.61786246]\n",
            "  [-0.17076135 -0.12737668]\n",
            "  [ 1.9682679   1.0113165 ]\n",
            "  [ 1.5570506  -0.50567806]\n",
            "  [-0.8496867  -0.11333856]\n",
            "  [ 0.46044916 -1.5148866 ]\n",
            "  [ 0.2603938  -0.667731  ]\n",
            "  [ 0.390239   -0.8183035 ]]\n",
            "\n",
            " [[ 0.5116496   0.7938992 ]\n",
            "  [-0.36580473  0.28036124]\n",
            "  [ 1.619271    1.4375819 ]\n",
            "  [ 1.0190289   1.8487402 ]\n",
            "  [ 2.114327    1.1017504 ]\n",
            "  [ 0.32008412  1.1586103 ]\n",
            "  [-0.48265496  0.48894656]\n",
            "  [ 1.4943178  -2.0454373 ]\n",
            "  [-0.13122469 -1.0729203 ]\n",
            "  [ 0.9561659  -2.2863739 ]]\n",
            "\n",
            " [[ 1.5482113  -1.0610366 ]\n",
            "  [ 0.2791086   1.4410967 ]\n",
            "  [ 2.2852516   1.4669932 ]\n",
            "  [ 1.0136163  -2.0550616 ]\n",
            "  [ 1.9892963  -0.36941808]\n",
            "  [-0.7006393  -0.95297533]\n",
            "  [ 1.7611092  -0.73666084]\n",
            "  [ 0.990804   -0.23713636]\n",
            "  [ 1.8747833  -1.1969779 ]\n",
            "  [ 0.68387496  1.1049365 ]]\n",
            "\n",
            " [[ 1.2032396  -0.57594335]\n",
            "  [-0.9977772  -1.5189624 ]\n",
            "  [ 1.0171599  -1.0731907 ]\n",
            "  [ 0.7865189  -0.00354534]\n",
            "  [ 2.8683107   0.72493935]\n",
            "  [-0.48836598 -1.5167868 ]\n",
            "  [ 0.9632351   0.02702831]\n",
            "  [-0.7184152  -2.423151  ]\n",
            "  [-1.1385088  -0.4629494 ]\n",
            "  [ 0.7818752  -0.6575927 ]]]\n"
          ]
        }
      ],
      "source": [
        "def train_probabilistic_model(model, x_train, y_train, epochs=50):\n",
        "    \"\"\"\n",
        "    Train probabilistic model with negative log-likelihood loss\n",
        "    \"\"\"\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step(x_batch, y_batch):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Model outputs distribution\n",
        "            pred_dist = model(x_batch, training=True)\n",
        "            \n",
        "            # Negative log-likelihood loss\n",
        "            nll_loss = -tf.reduce_mean(pred_dist.log_prob(y_batch))\n",
        "            \n",
        "            # Add regularization\n",
        "            reg_loss = sum(model.losses) if model.losses else 0.\n",
        "            total_loss = nll_loss + 0.01 * reg_loss\n",
        "        \n",
        "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "        return total_loss, nll_loss\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, nll_loss = train_step(x_train, y_train)\n",
        "        \n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch:2d}: Total Loss = {total_loss:.4f}, \"\n",
        "                  f\"NLL = {nll_loss:.4f}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Generate synthetic regression data\n",
        "n_samples = 1000\n",
        "x_train = tf.random.normal([n_samples, 5])\n",
        "true_weights = tf.constant([[1., -0.5], [0.8, 1.2], [0., 0.5], [-0.3, 0.], [0.7, -0.8]])\n",
        "y_train = x_train @ true_weights + tf.random.normal([n_samples, 2]) * 0.1\n",
        "\n",
        "# Train the model\n",
        "print(\"Training probabilistic regressor...\")\n",
        "trained_model = train_probabilistic_model(model, x_train, y_train, epochs=100)\n",
        "\n",
        "# Make probabilistic predictions\n",
        "x_test = tf.random.normal([10, 5])\n",
        "pred_dist = trained_model(x_test)\n",
        "\n",
        "print(f\"\\nPredictions with uncertainty:\")\n",
        "print(f\"Mean predictions shape: {pred_dist.mean().shape}\")\n",
        "print(f\"Std predictions shape: {pred_dist.stddev().shape}\")\n",
        "print(f\"Sample predictions:\\n{pred_dist.sample(5).numpy()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. **Expert Applications**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 **Bayesian Neural Networks with Trainable Priors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Bayesian Neural Network...\n",
            "Step 0: MSE = 26.2266, KL = 42.2583\n",
            "Step 100: MSE = 24.5402, KL = 42.2583\n",
            "Step 200: MSE = 30.4602, KL = 42.2583\n",
            "Step 300: MSE = 30.4573, KL = 42.2583\n",
            "Step 400: MSE = 25.6658, KL = 42.2583\n",
            "\n",
            "Predictive uncertainty shape: (100, 1)\n",
            "Average uncertainty: 0.6459\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfd = tfp.distributions\n",
        "\n",
        "class BayesianLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Bayesian neural network layer with trainable weight distributions\n",
        "    \"\"\"\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        \n",
        "        # Prior parameters (trainable)\n",
        "        self.weight_loc = tf.Variable(\n",
        "            tf.random.normal([input_dim, self.units], stddev=0.1),\n",
        "            name='weight_loc'\n",
        "        )\n",
        "        self.weight_raw_scale = tf.Variable(\n",
        "            tf.random.normal([input_dim, self.units], stddev=0.1) - 1,\n",
        "            name='weight_raw_scale'\n",
        "        )\n",
        "        \n",
        "        self.bias_loc = tf.Variable(\n",
        "            tf.zeros([self.units]),\n",
        "            name='bias_loc'\n",
        "        )\n",
        "        self.bias_raw_scale = tf.Variable(\n",
        "            tf.random.normal([self.units], stddev=0.1) - 1,\n",
        "            name='bias_raw_scale'\n",
        "        )\n",
        "        \n",
        "        super().build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        # Create weight and bias distributions\n",
        "        weight_scale = tf.nn.softplus(self.weight_raw_scale) + 1e-6\n",
        "        bias_scale = tf.nn.softplus(self.bias_raw_scale) + 1e-6\n",
        "        \n",
        "        weight_dist = tfd.Normal(loc=self.weight_loc, scale=weight_scale)\n",
        "        bias_dist = tfd.Normal(loc=self.bias_loc, scale=bias_scale)\n",
        "        \n",
        "        # Sample weights and biases\n",
        "        weights = weight_dist.sample()\n",
        "        biases = bias_dist.sample()\n",
        "        \n",
        "        # Standard linear transformation\n",
        "        output = tf.matmul(inputs, weights) + biases\n",
        "        \n",
        "        if training:\n",
        "            # Add KL divergence to losses for regularization\n",
        "            prior_weight = tfd.Normal(loc=0., scale=1.)\n",
        "            prior_bias = tfd.Normal(loc=0., scale=1.)\n",
        "            \n",
        "            weight_kl = tf.reduce_sum(tfd.kl_divergence(weight_dist, prior_weight))\n",
        "            bias_kl = tf.reduce_sum(tfd.kl_divergence(bias_dist, prior_bias))\n",
        "            \n",
        "            self.add_loss(weight_kl + bias_kl)\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "# Bayesian neural network model\n",
        "class BayesianNN(tf.keras.Model):\n",
        "    def __init__(self, hidden_units=50, output_units=1):\n",
        "        super().__init__()\n",
        "        self.hidden = BayesianLayer(hidden_units)\n",
        "        self.output_layer = BayesianLayer(output_units)\n",
        "    \n",
        "    def call(self, inputs, training=None):\n",
        "        x = tf.nn.relu(self.hidden(inputs, training=training))\n",
        "        return self.output_layer(x, training=training)\n",
        "\n",
        "\n",
        "# Create and demonstrate Bayesian NN\n",
        "bayesian_model = BayesianNN(hidden_units=20, output_units=1)\n",
        "\n",
        "# Generate data\n",
        "x_data = tf.linspace(-3., 3., 100)[:, None]\n",
        "y_data = 0.5 * x_data**3 + tf.random.normal([100, 1]) * 0.3\n",
        "\n",
        "# BUILD THE MODEL - This is crucial!\n",
        "_ = bayesian_model(x_data[:1], training=True)\n",
        "\n",
        "print(\"Training Bayesian Neural Network...\")\n",
        "optimizer = tf.keras.optimizers.Adam(0.01)\n",
        "\n",
        "for step in range(500):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Clear accumulated losses from previous iterations\n",
        "        bayesian_model.losses.clear()\n",
        "        \n",
        "        pred = bayesian_model(x_data, training=True)\n",
        "        mse_loss = tf.reduce_mean((pred - y_data)**2)\n",
        "        kl_loss = sum(bayesian_model.losses)\n",
        "        total_loss = mse_loss + 0.01 * kl_loss\n",
        "    \n",
        "    grads = tape.gradient(total_loss, bayesian_model.trainable_variables)\n",
        "    \n",
        "    # Filter out None gradients and create valid gradient-variable pairs\n",
        "    grads_and_vars = [\n",
        "        (grad, var) for grad, var in zip(grads, bayesian_model.trainable_variables) \n",
        "        if grad is not None\n",
        "    ]\n",
        "    \n",
        "    # Only apply gradients if we have valid gradients\n",
        "    if grads_and_vars:\n",
        "        optimizer.apply_gradients(grads_and_vars)\n",
        "    \n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step}: MSE = {mse_loss:.4f}, KL = {kl_loss:.4f}\")\n",
        "\n",
        "# Uncertainty quantification\n",
        "predictions = []\n",
        "for _ in range(100):\n",
        "    pred = bayesian_model(x_data, training=False)\n",
        "    predictions.append(pred)\n",
        "\n",
        "predictions = tf.stack(predictions)\n",
        "mean_pred = tf.reduce_mean(predictions, axis=0)\n",
        "std_pred = tf.math.reduce_std(predictions, axis=0)\n",
        "\n",
        "print(f\"\\nPredictive uncertainty shape: {std_pred.shape}\")\n",
        "print(f\"Average uncertainty: {tf.reduce_mean(std_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 **Generative Models with Trainable Distributions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAE Loss components:\n",
            "  Total: 547.0978\n",
            "  Reconstruction: 544.7975\n",
            "  KL Divergence: 2.3003\n"
          ]
        }
      ],
      "source": [
        "class SimpleVAE(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Variational Autoencoder with trainable encoder and decoder distributions\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=10, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder_hidden = tf.keras.layers.Dense(50, activation='relu')\n",
        "        self.encoder_loc = tf.keras.layers.Dense(latent_dim)\n",
        "        self.encoder_raw_scale = tf.keras.layers.Dense(latent_dim)\n",
        "        \n",
        "        # Decoder  \n",
        "        self.decoder_hidden = tf.keras.layers.Dense(50, activation='relu')\n",
        "        self.decoder_logits = tf.keras.layers.Dense(784)  # For MNIST-like data\n",
        "    \n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode input to latent distribution parameters\"\"\"\n",
        "        h = self.encoder_hidden(x)\n",
        "        loc = self.encoder_loc(h)\n",
        "        raw_scale = self.encoder_raw_scale(h)\n",
        "        scale = tf.nn.softplus(raw_scale) + 1e-6\n",
        "        \n",
        "        return tfd.MultivariateNormalDiag(loc=loc, scale_diag=scale)\n",
        "    \n",
        "    def decode(self, z):\n",
        "        \"\"\"Decode latent samples to reconstruction distribution\"\"\"\n",
        "        h = self.decoder_hidden(z)\n",
        "        logits = self.decoder_logits(h)\n",
        "        \n",
        "        return tfd.Independent(\n",
        "            tfd.Bernoulli(logits=logits),\n",
        "            reinterpreted_batch_ndims=1\n",
        "        )\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        # Encode\n",
        "        posterior = self.encode(inputs)\n",
        "        \n",
        "        # Sample from posterior\n",
        "        z = posterior.sample()\n",
        "        \n",
        "        # Decode\n",
        "        reconstruction_dist = self.decode(z)\n",
        "        \n",
        "        return posterior, reconstruction_dist\n",
        "    \n",
        "    def compute_loss(self, x):\n",
        "        \"\"\"Compute VAE loss (ELBO)\"\"\"\n",
        "        posterior, reconstruction_dist = self(x)\n",
        "        \n",
        "        # Sample from posterior for reconstruction\n",
        "        z = posterior.sample()\n",
        "        \n",
        "        # Reconstruction loss\n",
        "        reconstruction_loss = -tf.reduce_mean(reconstruction_dist.log_prob(x))\n",
        "        \n",
        "        # KL divergence\n",
        "        prior = tfd.MultivariateNormalDiag(\n",
        "            loc=tf.zeros(self.latent_dim),\n",
        "            scale_diag=tf.ones(self.latent_dim)\n",
        "        )\n",
        "        kl_loss = tf.reduce_mean(tfd.kl_divergence(posterior, prior))\n",
        "        \n",
        "        # ELBO = -reconstruction_loss - kl_loss (we minimize negative ELBO)\n",
        "        return reconstruction_loss + kl_loss, reconstruction_loss, kl_loss\n",
        "\n",
        "# Example usage (with dummy data)\n",
        "vae = SimpleVAE(latent_dim=5)\n",
        "dummy_data = tf.random.uniform([64, 784], maxval=1.0)  # Batch of binary images\n",
        "\n",
        "total_loss, recon_loss, kl_loss = vae.compute_loss(dummy_data)\n",
        "print(f\"VAE Loss components:\")\n",
        "print(f\"  Total: {total_loss:.4f}\")\n",
        "print(f\"  Reconstruction: {recon_loss:.4f}\")\n",
        "print(f\"  KL Divergence: {kl_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. **Complete Reference Guide**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 **Trainable Distribution Creation Patterns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === BASIC TRAINABLE DISTRIBUTIONS ===\n",
        "# Single trainable parameter\n",
        "tfd.Normal(loc=tf.Variable(0.), scale=1.)\n",
        "tfd.Exponential(rate=tf.Variable(1.))\n",
        "tfd.Beta(concentration1=tf.Variable(1.), concentration0=tf.Variable(1.))\n",
        "\n",
        "# Multiple trainable parameters with constraints\n",
        "def safe_normal():\n",
        "    return tfd.Normal(\n",
        "        loc=tf.Variable(0.),\n",
        "        scale=tf.nn.softplus(tf.Variable(0.)) + 1e-6\n",
        "    )\n",
        "\n",
        "# === ADVANCED TRAINABLE PATTERNS ===\n",
        "# Using TransformedVariable for constraints\n",
        "tfd.Normal(\n",
        "    loc=tf.Variable(0.),\n",
        "    scale=tfp.util.TransformedVariable(\n",
        "        1., bijector=tfp.bijectors.Exp(), name='scale'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Multivariate with trainable parameters\n",
        "def trainable_multivariate_normal(event_size):\n",
        "    return tfd.MultivariateNormalDiag(\n",
        "        loc=tf.Variable(tf.zeros(event_size)),\n",
        "        scale_diag=tf.nn.softplus(tf.Variable(tf.zeros(event_size))) + 1e-6\n",
        "    )\n",
        "\n",
        "# === NEURAL NETWORK INTEGRATION ===\n",
        "# Probabilistic layer outputs\n",
        "def probabilistic_output_layer(inputs, output_size):\n",
        "    loc = tf.keras.layers.Dense(output_size)(inputs)\n",
        "    raw_scale = tf.keras.layers.Dense(output_size)(inputs)\n",
        "    scale = tf.nn.softplus(raw_scale) + 1e-6\n",
        "    \n",
        "    return tfd.Independent(\n",
        "        tfd.Normal(loc=loc, scale=scale),\n",
        "        reinterpreted_batch_ndims=1\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 **Training Patterns Reference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === BASIC TRAINING SETUP ===\n",
        "def setup_basic_training(distribution, data):\n",
        "    optimizer = tf.keras.optimizers.Adam(0.01)\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step():\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = -tf.reduce_mean(distribution.log_prob(data))\n",
        "        grads = tape.gradient(loss, distribution.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, distribution.trainable_variables))\n",
        "        return loss\n",
        "    \n",
        "    return train_step\n",
        "\n",
        "# === ADVANCED TRAINING WITH REGULARIZATION ===\n",
        "def setup_regularized_training(model, data, kl_weight=0.01):\n",
        "    optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "    \n",
        "    @tf.function\n",
        "    def train_step():\n",
        "        with tf.GradientTape() as tape:\n",
        "            dist = model(data, training=True)\n",
        "            nll_loss = -tf.reduce_mean(dist.log_prob(data))\n",
        "            reg_loss = sum(model.losses)\n",
        "            total_loss = nll_loss + kl_weight * reg_loss\n",
        "        \n",
        "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        return total_loss, nll_loss, reg_loss\n",
        "    \n",
        "    return train_step\n",
        "\n",
        "# === CUSTOM LOSS FUNCTIONS ===\n",
        "class NegativeLogLikelihood(tf.keras.losses.Loss):\n",
        "    def call(self, y_true, y_pred_dist):\n",
        "        return -tf.reduce_mean(y_pred_dist.log_prob(y_true))\n",
        "\n",
        "class ELBOLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, kl_weight=1.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.kl_weight = kl_weight\n",
        "    \n",
        "    def call(self, y_true, outputs):\n",
        "        reconstruction_dist, posterior, prior = outputs\n",
        "        \n",
        "        # Reconstruction term\n",
        "        recon_loss = -tf.reduce_mean(reconstruction_dist.log_prob(y_true))\n",
        "        \n",
        "        # KL divergence\n",
        "        kl_loss = tf.reduce_mean(tfd.kl_divergence(posterior, prior))\n",
        "        \n",
        "        return recon_loss + self.kl_weight * kl_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 **Common Parameter Constraints**\n",
        "\n",
        "| **Parameter Type** | **Constraint** | **Implementation** | **Use Case** |\n",
        "|-------------------|----------------|-------------------|--------------|\n",
        "| **Scale/Std** | Positive | `tf.nn.softplus(raw) + 1e-6` | Normal, Exponential |\n",
        "| **Probability** | [4] | `tf.nn.sigmoid(raw)` | Bernoulli, Beta |\n",
        "| **Concentration** | Positive | `tf.nn.softplus(raw) + 1e-6` | Gamma, Dirichlet |\n",
        "| **Rate** | Positive | `tf.exp(raw)` | Exponential, Poisson |\n",
        "| **Correlation** | [-1,1] | `tf.nn.tanh(raw)` | Correlation matrices |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° **Final Notes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **The Trainable Revolution**: Trainable distributions represent the **convergence of classical statistics and modern deep learning**. They transform static probability models into dynamic, learnable components that can discover patterns in data automatically.\n",
        "\n",
        "- **Parameter Constraints Are Critical**: Always ensure parameters satisfy their mathematical constraints (e.g., scales > 0, probabilities in ). Use appropriate transformations like `softplus`, `sigmoid`, or `TransformedVariable`.\n",
        "\n",
        "- **Gradient Flow**: Trainable distributions enable **end-to-end differentiable probabilistic models**. The gradient flows through sampling operations (with reparameterization) and log_prob evaluations seamlessly.\n",
        "\n",
        "- **Loss Function Design**: Negative log-likelihood is your fundamental loss. Add regularization (KL divergence, priors) to prevent overfitting and encourage meaningful parameter values.\n",
        "\n",
        "- **Numerical Stability**: Always add small epsilon values (`1e-6`) to transformed parameters to avoid numerical issues. Monitor gradients for NaN or explosion.\n",
        "\n",
        "- **The Bayesian Advantage**: Trainable distributions naturally incorporate uncertainty. Unlike deterministic models that give point estimates, trainable distributions provide **full uncertainty quantification**.\n",
        "\n",
        "- **Start Simple, Scale Complex**: Begin with single-parameter distributions, then progress to multivariate, then neural integration, then Bayesian networks. Each level builds on the previous.\n",
        "\n",
        "- **Memory Considerations**: Trainable distributions can be memory-intensive during training due to gradient computation. Use techniques like gradient accumulation for large models.\n",
        "\n",
        ">Trainable distributions are the **gateway to modern probabilistic deep learning**. They enable models that not only make predictions but also quantify their confidence‚Äîessential for high-stakes applications where uncertainty matters.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM7ntyEtSjBT9Pp9UcmM8aT",
      "collapsed_sections": [
        "CG5Au3kC1liV",
        "2v2aM6Wa1v_A"
      ],
      "include_colab_link": true,
      "name": "00_The_TensorFlow_Probability_library.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Probabilistic-Deep-Learning-with-TensorFlow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
